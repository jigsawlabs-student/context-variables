{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "seven-family",
   "metadata": {},
   "source": [
    "# Context Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chinese-companion",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "editorial-chest",
   "metadata": {},
   "source": [
    "Now that we know how to extract data from RDS, and move it to S3, to eventually move it to airflow, let's see how we can make sure we are not copying over data we have already moved.  To do so, we need to know information about the last time we have selected our data.\n",
    "\n",
    "Airflow gives us information about the specific operations through context variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-spanish",
   "metadata": {},
   "source": [
    "### Context Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pediatric-transaction",
   "metadata": {},
   "source": [
    "We can use context variables in airflow with something like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-signature",
   "metadata": {},
   "source": [
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def extract_sql(*args, **kwargs):\n",
    "    return f\"Date executed: {kwargs['execution_date']}\"\n",
    "\n",
    "dag = DAG(dag_id = 'etl_dag', start_date = datetime.now() - timedelta(days = 1))\n",
    "task = PythonOperator(\n",
    "    task_id='sql_task',\n",
    "    python_callable=extract_sql,\n",
    "    provide_context=True,\n",
    "    dag=dag)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eight-replication",
   "metadata": {},
   "source": [
    "So as we see from the above, context variables provide access to metadata about the task that is run.  To access this metadata, we set `provide_context` to `True` when creating the task.  And then we are given a dictionary of metadata that we access through the `kwargs` argument of the python callable.\n",
    "\n",
    "> In the example above, we decide to access the execution date."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legislative-internship",
   "metadata": {},
   "source": [
    "If we boot up the dag, and check the logs we'll see something like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superb-brook",
   "metadata": {},
   "source": [
    "> <img src=\"./date_executed_1.png\" width=\"90%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-brunswick",
   "metadata": {},
   "source": [
    "> So we can ssee that the date executed is in the return value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sound-pledge",
   "metadata": {},
   "source": [
    "Now even more useful to us than the date executed is the previous execution date."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "future-allowance",
   "metadata": {},
   "source": [
    "```python\n",
    "def extract_sql(*args, **kwargs):\n",
    "    return f\"\"\"Current Date: {kwargs['execution_date']}, previous date: {kwargs['prev_execution_date']}\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peaceful-entry",
   "metadata": {},
   "source": [
    "Having access to the previouss execuction date is valuable because when moving over our data to s3, we can first only select data created after the previous task was run:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fresh-neighbor",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "def extract_sql(*args, **kwargs):\n",
    "    rds_hook = PostgresHook('rds')\n",
    "    s3_hook = S3Hook('s3_connection')\n",
    "    query = f\"\"\"SELECT * FROM zipcodes WHERE created_at > {kwargs['prev_execution_date']};\"\"\"\n",
    "    zipcode_records = rds_hook.get_records(query)\n",
    "    \n",
    "    mem_file = io.StringIO()\n",
    "    csv_writer = csv.writer(mem_file, lineterminator=os.linesep)\n",
    "    csv_writer.writerows(zipcode_records)\n",
    "    # encode into a byte stream\n",
    "    mem_file_binary = io.BytesIO(mem_file.getvalue().encode())\n",
    "    s3_hook.load_file_obj(\n",
    "       file_obj=mem_file_binary,\n",
    "       bucket_name='jigsaw-sample-data',\n",
    "       key=f\"zipcodes-{kwargs['ds']}.csv\",\n",
    "       replace=True,\n",
    "   )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-protection",
   "metadata": {},
   "source": [
    "So here, we can see that when selecting records, we select records after the previous execution date.  And then when we upload with the current execution data -- available via `ds`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endless-society",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "\n",
    "[Airflow Blog - Zen of Python](https://godatadriven.com/blog/the-zen-of-python-and-apache-airflow/)\n",
    "\n",
    "[Prev Execution Date](https://airflow.apache.org/docs/apache-airflow/stable/macros-ref.html)\n",
    "\n",
    "\n",
    "[Full Example Engineering Blog](https://medium.com/leboncoin-engineering-blog/data-traffic-control-with-apache-airflow-ab8fd3fc8638)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
